---
title: "SEAGULL: No-reference Image Quality Assessment for Regions of Interest via Vision-Language Instruction Tuning"
collection: publications
permalink: /publication/2024-11-15-SEAGULL
date: 2024-11-15
venue: 'Arxiv'
citation: '<b>Chen Z</b>, Wang J, Wang W, et al. SEAGULL: No-reference Image Quality Assessment for Regions of Interest via Vision-Language Instruction Tuning[J]. arXiv preprint arXiv:2411.10161, 2024.'
codeurl: 'https://github.com/chencn2020/SEAGULL'
paperdate: 11/2024
paperYear: 2024
framework: 'https://chencn2020.github.io/images/paper/SEAGULL.png'
fast_read: 'Existing Image Quality Assessment (IQA) methods achieve remarkable success in analyzing quality for overall image, but few works explore quality analysis for Regions of Interest (ROIs). The quality analysis of ROIs can provide fine-grained guidance for image quality improvement and is crucial for scenarios focusing on region-level quality. This paper proposes a novel network, SEAGULL, which can SEe and Assess ROIs quality with GUidance from a Large vision-Language model. SEAGULL incorporates a vision-language model (VLM), masks generated by Segment Anything Model (SAM) to specify ROIs, and a meticulously designed Mask-based Feature Extractor (MFE) to extract global and local tokens for specified ROIs, enabling accurate fine-grained IQA for ROIs. Moreover, this paper constructs two ROI-based IQA datasets, SEAGULL-100w and SEAGULL-3k, for training and evaluating ROI-based IQA. SEAGULL-100w comprises about 100w synthetic distortion images with 33 million ROIs for pre-training to improve the model's ability of regional quality perception, and SEAGULL-3k contains about 3k authentic distortion ROIs to enhance the model's ability to perceive real world distortions. After pre-training on SEAGULL-100w and fine-tuning on SEAGULL-3k, SEAGULL shows remarkable performance on fine-grained ROI quality assessment.'
arxiv: 'https://arxiv.org/abs/2411.10161'
onlineDemo: 'https://huggingface.co/spaces/Zevin2023/SEAGULL'
dataset: 'https://huggingface.co/datasets/Zevin2023/SEAGULL-100w'
---

# Abstract 

With the rising demand for high-resolution (HR) images, No-Reference Image Quality Assessment (NR-IQA) gains more attention, as it can ecaluate image quality in real-time on mobile devices and enhance user experience. However, existing NR-IQA methods often resize or crop the HR images into small resolution, which leads to a loss of important details. And most of them are of high computational complexity, which hinders their application on mobile devices due to limited computational resources. To address these challenges, we propose MobileIQA, a novel approach that utilizes lightweight backbones to efficiently assess image quality while preserving image details through high-resolution input. MobileIQA employs the proposed multi-view attention learning (MAL) module to capture diverse opinions, simulating subjective opinions provided by different annotators during the dataset annotation process. The model uses a teacher model to guide the learning of a student model through knowledge distillation. This method significantly reduces computational complexity while maintaining high performance. Experiments demonstrate that MobileIQA outperforms novel IQA methods on evaluation metrics and computational efficiency.

<!-- # Introduction

# Experiments


# Others

[Download paper here](https://openaccess.thecvf.com/content/ACCV2022/papers/Chen_Teacher-Guided_Learning_for_Blind_Image_Quality_Assessment_ACCV_2022_paper.pdf)

Recommended citation:
```
@inproceedings{chen2022teacher,
  title={Teacher-Guided Learning for Blind Image Quality Assessment},
  author={Chen, Zewen and Wang, Juan and Li, Bing and Yuan, Chunfeng and Xiong, Weihua and Cheng, Rui and Hu, Weiming},
  booktitle={Proceedings of the Asian Conference on Computer Vision},
  pages={2457--2474},
  year={2022}
}
``` -->